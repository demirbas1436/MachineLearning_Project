# -*- coding: utf-8 -*-
"""Untitledtripadvisor8t.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9tXuxfCFyZsa6E2EnTDN2ZC2_GdhTXd
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from collections import defaultdict
from sklearn.model_selection import train_test_split, GridSearchCV
import nltk
from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer
from nltk.corpus import stopwords, wordnet
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, f1_score, recall_score, confusion_matrix, classification_report, make_scorer
from sklearn.decomposition import PCA
from sklearn import linear_model
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn import tree
from xgboost import XGBClassifier
from lightgbm import LGBMRegressor, LGBMClassifier, Booster
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
import plotly.express as px

df = pd.read_csv('/content/denem/cleaned_merged_data.csv')

"""**Exploratory Veri Analizi**"""

df.head()

from matplotlib import pyplot as plt
df['Rating'].plot(kind='line', figsize=(8, 4), title='Rating')
plt.gca().spines[['top', 'right']].set_visible(False)

df.isna().sum()

"""Bu kod her sütunda kaç tane eksik veri olduğunu gösteren bir seri döndürür.

"""

grouped = df.groupby('Rating')['Rating'].count()
sns.barplot(x = grouped.index, y = grouped.values, palette='mako').set_title('Review Count by Rating')

"""`grouped = df.groupby('Rating')['Rating'].count():` Bu kısım, df adlı DataFrame'deki Rating sütununa göre gruplama yapar ve her bir grup için kaç tane Rating değeri olduğunu sayar. Sonuç olarak, her bir Rating değeri ve bu değerin kaç kez geçtiğini belirten bir seri elde edilir.

Her bir derecelendirme için inceleme sayısını çizdikten sonra, sınıf dengesizliğinin olduğu açıktır. 1 ve 2 derecelerini, 3 ve 4 derecelerini ve 5 derecesini bir araya getirerek, sınıf dengesizliği azaltılmalıdır.
"""

def new_rating(num):
    if (num == 1) or (num == 2):
        return 'dusuk'
    elif (num == 3) or (num == 4):
        return 'ortalama'
    else:
        return 'harika'

df['new_rating'] = df['Rating'].apply(new_rating)

"""new_rating Fonksiyonu:

Fonksiyon, bir num parametresi alır.

  * num değeri 1 veya 2(rating değeri) ise 'dusuk' döner.

  * num değeri 3 veya 4(rating değeri) ise 'ortalama' döner.

  * Diğer tüm durumlarda 'harika' döner.


Bu şekilde, her Rating değeri için 'dusuk', 'ortalama' veya 'harika' şeklinde yeni sınıflandırmalar oluşturulur.
"""

new_rating_grouped = df.groupby('new_rating')['Rating'].count()
print(new_rating_grouped)
categories = ['dusuk', 'ortalama', 'harika']
counts = [new_rating_grouped.get(cat, 0) for cat in categories]

# Bar grafiğini çizme
plt.figure(figsize=(8, 6))
sns.barplot(x=categories, y=counts, palette='mako').set_title('Grouped Review Count by Rating')
plt.xlabel('New Rating Categories')
plt.ylabel('Review Count')
plt.show()

"""new_rating(dusuk, ortalama, harika) kategorilerine göre Rating sayısını gruplar ve sayar.

`new_rating_grouped.get(cat, 0):` Bu kısım, new_rating_grouped adlı sözlükte her kategori için sayıyı alır. Eğer kategori sözlükte yoksa, 0 değerini döner.

Sınıf dengesizliği şimdi daha az belirgin, ancak hâlâ daha az "kötü" inceleme türü var. Modellerde class_weight="balanced" parametresi kullanılarak bu durum dikkate alınacaktır.

**Kelime Sayısı Grafiği**
"""

df['words'] = [x.split() for x in df['Review Text']]
df['word_count'] = [len(x) for x in df['words']]

grouped_rating = df.groupby('new_rating')['word_count'].mean()
sns.barplot(x = grouped_rating.index, y = grouped_rating.values).set_title('Word Count by Review')

"""Bu grafik, kelime sayısı arttıkça incelemenin daha kötü olduğunu göstermektedir. Bu özelliği modelleme sürecine dahil etmeyi düşünmek faydalı olacaktır.

**Preprocessing**
"""

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
# Stopwords kaynaklarını indir
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

# Türkçe stopwords listesini yükle
sw = stopwords.words('turkish')

"""1. Gerekli Paketleri İndirme:


    nltk.download('punkt'): Bu, cümleleri ve kelimeleri bölmek için kullanılan bir paket.

    nltk.download('averaged_perceptron_tagger'): Kelimelerin dilbilgisel etiketlerini belirlemek için kullanılır (örneğin, isim, fiil).

    nltk.download('wordnet'): İngilizce kelimeler arasındaki anlam ilişkilerini içeren büyük bir veri tabanı.

    nltk.download('stopwords'): "ve", "bu" gibi sık kullanılan ama genellikle metin analizinde göz ardı edilen kelimelerin listesi.

2. Türkçe Stopwords Listesini Yükleme:

    sw = stopwords.words('turkish'): Bu komut, Türkçe dilinde sık kullanılan durdurma kelimelerinin bir listesini yükler.
"""

def get_wordnet_pos(treebank_tag):
    '''
    wordnet etiketlere donusturme
    '''
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

"""Bu fonksiyon, verilen dilbilgisel etiketleri (treebank_tag) WordNet etiketlerine dönüştürür.

1. İlk Karakter Kontrolü:

  Etiketin ilk karakterine bakar (startswith yöntemi).

  Dönüştürme:

  J ile başlıyorsa: Sıfat (ADJ) olarak döner.

  V ile başlıyorsa: Fiil (VERB) olarak döner.

  N ile başlıyorsa: İsim (NOUN) olarak döner.

  R ile başlıyorsa: Zarf (ADV) olarak döner.

  Hiçbiri değilse: Varsayılan olarak isim (NOUN) döner.

Bu şekilde, fonksiyon verilen dilbilgisel etiketleri uygun WordNet etiketlerine çevirir.
"""

pip install stanza

"""Stanza, doğal dil işleme projelerinde kapsamlı ve güçlü araçlar sunarak metin verilerinden anlam çıkarma süreçlerini kolaylaştırır."""

import stanza

# Türkçe modelin indirilmesi
stanza.download('tr')

"""Stanza kütüphanesinin Türkçe dil modeli verilerini indirir. Bu model, Türkçe metinleri işlemek için gerekli olan tüm bileşenleri (örneğin, tokenizasyon, etiketleme, ayrıştırma) içerir."""

import stanza
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

# Stanza Türkçe NLP modeli yükle
nlp = stanza.Pipeline('tr')

# Türkçe durdurma kelimeleri yükle
stopwords_tr = stopwords.words('turkish')

# Belge işleme fonksiyonu
def doc_preparer_stanza(doc, stop_words=stopwords_tr):
    """
    :param doc: İşlenecek belge (bir metin veya yorum)
    :param stop_words: Durdurma kelimeleri listesi (varsayılan Türkçe)
    :return: İşlenmiş belge:
        - Küçük harfe dönüştürülmüş,
        - Noktalama ve sayılardan temizlenmiş,
        - Durdurma kelimeler çıkarılmış,
        - Lemmatization uygulanmış metin.
    """
    # Noktalama işaretlerini ve sayıları kaldırma
    regex_token = RegexpTokenizer(r"([a-zA-ZçöğüşıİÇÖĞÜŞ]+(?:’[a-zçöğüşıİÇÖĞÜŞ]+)?)")
    tokens = regex_token.tokenize(str(doc))

    # Küçük harfe dönüştürme
    tokens = [word.lower() for word in tokens]

    # Durdurma kelimeleri çıkarma
    tokens = [word for word in tokens if word not in stop_words]

    # Stanza ile kök hâline getirme
    processed_doc = nlp(' '.join(tokens))
    lemmatized_words = [word.lemma for sentence in processed_doc.sentences for word in sentence.words if word.lemma is not None]

    return ' '.join(lemmatized_words)

"""Bu fonksiyon, verilen bir metni işleyerek küçük harfe dönüştürür, noktalama ve durdurma kelimelerini çıkarır ve kelimeleri kök hâline getirir."""

# Örnek kullanım
sample_review = "Bu ürünü gerçekten çok beğendim, ancak bazı sorunlar vardı."
processed_review = doc_preparer_stanza(sample_review)
print("İşlenmiş Yorum:", processed_review)

df['tokenized'] = df['Review Text'].apply(doc_preparer_stanza)

"""Bu kod, Review Text sütunundaki her metni doc_preparer_stanza fonksiyonuyla işleyerek elde edilen sonuçları tokenized adlı yeni bir sütunda saklar."""

df.head()

"""**Kelimelerde Frekans Dağılımı**"""

df['tokenized_words'] = [x.split() for x in df['tokenized']]
tokenized_words = df['tokenized_words'].to_list()

word_list = []
for x in tokenized_words:
    word_list.extend(x)
word_list
freq_dist_text = nltk.FreqDist(word_list)
plt.subplots(figsize=(20,12))
freq_dist_text.plot(30)

"""1. Tokenized Sütununu Kelime Listesine Çevirme:

  `df['tokenized_words'] = [x.split() for x in df['tokenized']]:` tokenized sütunundaki her metni kelimelere bölerek tokenized_words adlı yeni bir sütun oluşturur.

  `tokenized_words = df['tokenized_words'].to_list():` Bu sütunu bir listeye çevirir.

2. Kelime Listesini Oluşturma:

  `word_list = []:` Boş bir kelime listesi oluşturur.

  `for x in tokenized_words: word_list.extend(x)`: tokenized_words listesindeki her kelimeyi word_list adlı listeye ekler.

3. Kelime Frekans Dağılımı:

  `freq_dist_text = nltk.FreqDist(word_list):` word_list listesindeki kelimelerin frekans dağılımını hesaplar.

Bu frekans dağılımı, hangi kelimelerin daha yüksek bir sıklığa sahip olduğunu göstermektedir.
"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

def show_wordcloud(data, title = None):
    wordcloud = WordCloud(
        background_color='black',
        stopwords=stopwords,
        max_words=50,
        max_font_size=40,
        scale=3,
        random_state=1 # chosen at random by flipping a coin; it was heads
    ).generate(str(data))

    fig = plt.figure(1, figsize=(12, 12))
    plt.axis('off')
    if title:
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)

    plt.imshow(wordcloud)
    plt.show()

"""1. Durdurma Kelimelerinin Ayarlanması:

  `stopwords = set(STOPWORDS):` STOPWORDS seti, genellikle analizlerde göz ardı edilen sık kullanılan kelimeleri içerir.

2. `show_wordcloud` Fonksiyonu:

  Bu fonksiyon, kelime bulutu oluşturup gösterir.

  `data` parametresi, kelime bulutunun oluşturulacağı metni temsil eder.

  `title` parametresi, kelime bulutunun başlığını temsil eder (isteğe bağlı).

3. Kelime Bulutu Oluşturma:

  `WordCloud` nesnesi oluşturulur:

  `background_color=`'black': Arka plan rengi siyah.

  `stopwords=stopwords:` Durdurma kelimeleri ayarlanır.

  `max_words=50:` Maksimum 50 kelime gösterilir.

  `max_font_size=40:` Maksimum yazı tipi boyutu 40.

  `scale=3:` Ölçek 3 olarak ayarlanır.

  `random_state=1:` Rastgelelik için sabit bir durum ayarlanır.

  `generate(str(data)):` Verilen veriden kelime bulutu oluşturulur.
"""

for rating in list(df['new_rating'].unique()):
    show_wordcloud(df[df['new_rating']==rating]['tokenized'], title=rating)

"""Yukarıdaki kelime bulutları, her kategoride en sık görünen kelimeleri göstermektedir."""

# Gerekli kütüphaneleri yükleyin
import pandas as pd

# Stanza POS etiketleme kullanımı
def pos_tags_stanza(doc):
    """
    Stanza kullanarak bir metindeki POS etiketlerini çıkarır.
    """
    processed_doc = nlp(doc)
    return [word.upos for sentence in processed_doc.sentences for word in sentence.words]

# Yeni bir DataFrame oluşturma
df_eda = pd.DataFrame()

# Lemmatized metin (işlenmiş hâl)
df_eda['LEM'] = df['tokenized']

# POS etiketleri
df_eda['POS'] = df_eda['LEM'].apply(pos_tags_stanza)

# POS kategorilerinin yüzdeleri
df_eda['NOUN'] = df_eda.POS.apply(
    lambda x: sum(1 for pos in x if pos == 'NOUN') / len(x) if len(x) > 0 else 0)  # İsim yüzdesi
df_eda['ADJ'] = df_eda.POS.apply(
    lambda x: sum(1 for pos in x if pos == 'ADJ') / len(x) if len(x) > 0 else 0)   # Sıfat yüzdesi
df_eda['ADV'] = df_eda.POS.apply(
    lambda x: sum(1 for pos in x if pos == 'ADV') / len(x) if len(x) > 0 else 0)   # Zarf yüzdesi
df_eda['VERB'] = df_eda.POS.apply(
    lambda x: sum(1 for pos in x if pos == 'VERB') / len(x) if len(x) > 0 else 0)  # Fiil yüzdesi

# Yapısal özellikler
df_eda['CHAR'] = df['Review Text'].apply(lambda x: len(x))                        # Karakter sayısı
df_eda['WORD'] = df['Review Text'].apply(lambda x: len(x.split(' ')))             # Kelime sayısı
df_eda['SENT'] = df['Review Text'].apply(lambda x: len(x.split('.')))             # Cümle sayısı
df_eda['LEN'] = df_eda.CHAR / df_eda.WORD                                         # Ortalama kelime uzunluğu
df_eda['AVG'] = df_eda.WORD / df_eda.SENT                                         # Ortalama cümle uzunluğu

# Hedef değişken (target)
df_eda['target'] = df['new_rating']

# DataFrame'in son hâli
print(df_eda.head())

"""Bu kod bir metin veri kümesini işlemek ve belirli metrikleri hesaplamak için kullanılır.
  1. POS Etiketleme: Stanza kütüphanesi kullanılarak kelimelerin dil bilgisel etiketleri (POS tags) çıkarılıyor.

  2. Yeni Bir DataFrame Oluşturma: df_eda adında yeni bir veri çerçevesi oluşturuluyor.

  3. Lemmatized Metin: İşlenmiş metin (LEM) sütununa atanıyor.

  4. POS Etiketleri: LEM sütunundaki her kelimenin POS etiketleri çıkarılıyor ve POS sütununa atanıyor.

(POS etiketleme, doğal dil işleme (NLP) alanında bir metindeki her kelimenin dil bilgisel kategorisini belirlemek için kullanılır.)

    POS Kategorilerinin Yüzdeleri:

        NOUN: İsim yüzdesi

        ADJ: Sıfat yüzdesi

        ADV: Zarf yüzdesi

        VERB: Fiil yüzdesi

    Yapısal Özellikler:

        CHAR: Karakter sayısı

        WORD: Kelime sayısı

        SENT: Cümle sayısı

        LEN: Ortalama kelime uzunluğu

        AVG: Ortalama cümle uzunluğu

  5. Hedef Değişken (Target): new_rating sütunundan hedef değişken (target) oluşturuluyor.
"""

import seaborn as sns
import matplotlib.pyplot as plt

import math

# Görselleştirilecek sütunlar
cols = ['NOUN', 'ADJ', 'ADV', 'VERB', 'CHAR', 'WORD', 'SENT', 'LEN', 'AVG']

# Alt alta grafik düzeni için ayarlar
fig, ax = plt.subplots(nrows=len(cols), ncols=1, figsize=(10, len(cols) * 4), facecolor='w')
plt.subplots_adjust(hspace=0.5)  # Grafikler arasındaki boşluk

sns.set_style('whitegrid')
sns.set_context("talk")

# Her sütun için grafik çizimi
for idx, col in enumerate(cols):
    sns.violinplot(
        data=df_eda,
        y=df_eda[col],
        x=df_eda['target'],
        ax=ax[idx],
        inner=None,
        palette='mako',
        dodge=False
    ).set_title(col)

    # Eksen ayarları
    ax[idx].set_xticks([])
    ax[idx].set_xlabel('')
    ax[idx].set_ylabel('')

# Grafikleri göster
plt.show()

"""Yukarıdaki grafik, her hedef kategoriye göre çeşitli konuşma bölümlerini ve diğer inceleme özelliklerini göstermektedir. Dikkat edilmesi gereken önemli bir nokta, kelime sayısı, karakter sayısı ve ortalama cümle uzunluğunun 'dusuk' incelemelerde diğerlerine göre daha yüksek olduğudur.

**Correlation Heatmap**
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Korelasyon hesaplaması için uygun sütunları seçme (sayılar içeren sütunlar)
numeric_cols = df_eda.select_dtypes(include=np.number).columns.tolist()

# Korelasyon matrisini hesaplama
correlation_matrix = df_eda[numeric_cols].corr()

# Üçgen maske oluşturma (alt üçgeni gizleme)
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Grafik ayarları
sns.set_style('white')
sns.set_palette('dark:salmon')
sns.set_context("talk")

# Isı haritası çizimi
plt.figure(figsize=(11, 11), facecolor='w')
sns.heatmap(
    correlation_matrix,
    mask=mask,
    vmin=-0.5, vmax=1, center=0,
    square=True,
    linewidths=1,
    cmap="mako",
    cbar_kws={"shrink": 0.5}
).set_title('Feature-to-Feature Correlation') # Özellik-Özellik Korelasyonu

plt.show()

"""Bu özellikten özelliğe korelasyon ısı haritasına dayanarak, karakter sayısı, kelime sayısı ve ortalama cümle uzunluğunun birbirleriyle yüksek bir korelasyona sahip olduğu görülmektedir.

**Vektörleştirme(Vectorizing)**

TF-IDF Vektörleştirici kullanılmıştır; bu, bir kelimenin bir belgede ne kadar sık göründüğünü ve ayrıca kelimenin genel korpustaki ne kadar benzersiz olduğunu dikkate alır. En anlamlı kelimeleri yakalayabilmek için, belgelerdeki kelimelerin en üst %20'sini ve en alt %10'unu kesiyoruz.

("Korpus" terimi,  belirli bir amaca yönelik olarak toplanmış  metin veri kümesini ifade eder.)

**Train Test Belirlenmesi**
"""

X = df['tokenized']
y = df['new_rating']

X_train, X_test, y_train, y_test =  train_test_split(X, y, train_size= .8, random_state= 42)

"""  `train_size=0.8:` Verilerin %80'ini eğitim seti, %20'sini test seti olarak ayırır.

  `random_state=42:` Rastgelelik için sabit bir başlangıç durumu belirler, böylece aynı bölme her seferinde tekrar edilebilir.


  `X_train:` Eğitim için kullanılacak özellikler.

  `X_test:` Test için kullanılacak özellikler.

  `y_train:` Eğitim için kullanılacak hedef değişken.

  `y_test:` Test için kullanılacak hedef değişken.
"""

tfidf_train = TfidfVectorizer(sublinear_tf=True, max_df=.9, min_df=.05,  ngram_range=(1, 1))

"""TfidfVectorizer kullanarak TF-IDF vektörleştirici oluşturur. İşte kısa bir açıklama:

  `sublinear_tf=True:` TF (Term Frequency) hesaplaması, logaritmik bir dönüşümle uygulanır.

  `max_df=0.9:` Belirli bir kelimenin, dokümanların %90'ından fazlasında bulunması durumunda o kelime filtrelenir (önemli olmayan yaygın kelimeleri çıkarmak için).

  `min_df=0.05:` Belirli bir kelimenin, dokümanların en az %5'inde bulunması durumunda o kelime dahil edilir (çok nadir kelimeleri çıkarmak için).

  `ngram_range=(1, 1):` Sadece tekli kelimeler (unigram) dikkate alınır.(Bu, metin analizinde her bir kelimenin ayrı ayrı inceleneceği anlamına gelir.)

Bu vektörleştirici, metin verinizi analiz ederken en anlamlı ve ayırt edici kelimeleri bulmak için kullanılır.

TF-IDF (Term Frequency-Inverse Document Frequency): Amacı, bir kelimenin bir belgedeki önemini değerlendirmektir.

    Term Frequency (TF): Bir kelimenin belirli bir belgede kaç kez geçtiğini ölçer. Daha sık geçen kelimeler, belgede daha önemli olarak kabul edilir.

    Inverse Document Frequency (IDF): Bir kelimenin tüm belgeler korpusundaki nadirliğini ölçer. Daha nadir geçen kelimeler, daha önemli olarak kabul edilir.

    TF-IDF: TF ve IDF değerlerinin çarpımı, bir kelimenin belirli bir belgede ne kadar önemli olduğunu belirler.

Özetle, TF-IDF, bir kelimenin bir belgede ne kadar önemli olduğunu belirlemek için kullanılan bir yöntemdir.
"""

train_features = tfidf_train.fit_transform(X_train).toarray()
test_features = tfidf_train.transform(X_test).toarray()

"""Bu kod, eğitim ve test verilerini TF-IDF vektörlerine dönüştürmek için kullanılır.
  `train_features = tfidf_train.fit_transform(X_train).toarray():`

        tfidf_train.fit_transform(X_train): Eğitim verilerini (X_train) TF-IDF vektörlerine dönüştürür ve modelin bu verilere göre öğrenmesini sağlar.

        .toarray(): Vektörleştirilmiş sonuçları bir numpy dizisine (array) dönüştürür.

        train_features: Eğitim verilerinin TF-IDF vektörlerine dönüştürülmüş hali.

  `test_features = tfidf_train.transform(X_test).toarray():`

        tfidf_train.transform(X_test): Test verilerini (X_test) TF-IDF vektörlerine dönüştürür, ancak bu sefer modeli yeniden öğrenmeden sadece dönüştürme işlemi yapar.

        .toarray(): Vektörleştirilmiş sonuçları bir numpy dizisine (array) dönüştürür.

        test_features: Test verilerinin TF-IDF vektörlerine dönüştürülmüş hali.
"""

pd.set_option('display.max_rows', None)

wm = tfidf_train.fit_transform(X_train)
tokens = tfidf_train.get_feature_names_out()


wm.shape
doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]
data = pd.DataFrame(data=wm.toarray(), index=doc_names,
                  columns=tokens)
data.mean().sort_values()

"""Bu kod, TF-IDF vektörleştirici kullanarak kelime frekanslarını analiz etmek için işlemler gerçekleştirir.

  `pd.set_option('display.max_rows', None):` DataFrame içindeki tüm satırların görüntülenmesini sağlar.

  `wm = tfidf_train.fit_transform(X_train):` Eğitim verilerini (X_train) TF-IDF vektörlerine dönüştürür ve wm adında bir sparse matris oluşturur.

  `tokens = tfidf_train.get_feature_names_out():` TF-IDF vektörleştirici tarafından tanınan tüm kelimeleri (tokens) alır.

  `vm.shape` = wm matrisinin boyutlarını (satır ve sütun sayısı) verir.

  `doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]:` Her bir belge için bir ad listesi (doc_names) oluşturur.

  `data = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=tokens):` wm matrisini bir DataFrame'e dönüştürür, belgeleri satır ve kelimeleri sütun olarak ayarlar.

  `data.mean().sort_values():` Her kelimenin ortalama TF-IDF skorunu hesaplar ve bu skorları küçükten büyüğe doğru sıralar.

Bu kod, eğitim verilerini TF-IDF vektörlerine dönüştürüp analiz ederek, metindeki kelime frekanslarının dağılımını anlamaya yardımcı olur.

Örneğin:

    "et" kelimesinin ortalama TF-IDF skoru 0.111438, bu da onun eğitim setindeki en yüksek öneme sahip kelimelerden biri olduğunu gösterir.

    "yemek" kelimesi 0.086314 skoru ile ikinci sırada yer almaktadır.

**PCA**

Sonuçların değişip değişmeyeceğini belirlemek için bazı modellerde çok boyutluluğu azaltmak amacıyla PCA (Ana Bileşen Analizi) kullandım.

PCA (Principal Component Analysis - Ana Bileşen Analizi),

    Boyut Azaltma: Yüksek boyutlu verileri daha az bileşene indirger ve bu sayede veri setinin boyutunu küçültür. Bu, hesaplama süresini ve kaynak kullanımını azaltır.

    Görselleştirme: Yüksek boyutlu verilerin iki veya üç boyutta görselleştirilmesini sağlar, böylece veri yapısını ve ilişkilerini daha iyi anlayabilirsiniz.

    Gürültü Azaltma: Veri setindeki gürültüyü (gereksiz veya hatalı veriler) azaltarak daha temiz ve anlamlı veriler elde edilmesine yardımcı olur.

    Özellik Seçimi: Orijinal veri setindeki en önemli özellikleri (bileşenleri) belirler, böylece modelin genel performansını iyileştirir.

    Aşırı Uyum (Overfitting) Azaltma: Özellikle küçük veri setlerinde, daha az önemli özellikleri çıkardığı için modelin aşırı uyum yapma riskini azaltır.

    Korelasyonlu Özellikleri Ele Alma: Veri setindeki özellikler arasındaki korelasyonu ortadan kaldırarak bağımsız bileşenler oluşturur.

PCA, veri analizinin yanı sıra, makine öğrenimi modellerinin performansını artırmak ve veriyi daha anlaşılır hale getirmek için kullanılabilir.
"""

pca = PCA(n_components=0.9, random_state=1)

pca_train = pca.fit_transform(train_features)
pca_test = pca.transform(test_features)

"""`pca = PCA(n_components=0.9, random_state=1):`

  `n_components=0.9`: Toplam varyansın %90'ını açıklayan bileşen sayısını
seçer. Bu, PCA'nın varyansın %90'ını koruyarak veri setinin boyutlarını azaltacağı anlamına gelir.

`pca_train = pca.fit_transform(train_features):`

  `pca.fit_transform(train_features):` Eğitim verilerini (train_features) kullanarak PCA modelini öğrenir ve bu verileri yeni bileşene dönüştürür.

pca_test = pca.transform(test_features):

"""

pca.n_components_

"""ca.n_components_ PCA (Principal Component Analysis): modelinin kaç tane ana bileşen seçtiğini gösterir. Bu değer, modelin veri setindeki toplam varyansın ne kadarını açıkladığını belirler. n_components=0.9 olarak ayarladığınında, PCA toplam varyansın %90'ını açıklayan yeterli sayıda bileşeni seçecektir."""

pca = PCA(n_components=3)
pca_result = pca.fit_transform(train_features)

pca1 = pca_result[:,0]
pca2 = pca_result[:,1]
pca3 = pca_result[:,2]

target = pd.Categorical(y_train).codes

"""Bu kod, PCA (Principal Component Analysis) kullanarak eğitim verilerini üç ana bileşene indirger ve bu bileşenleri ayrı ayrı saklar.
  
  `pca = PCA(n_components=3):` Üç ana bileşen seçer.

  `pca_result = pca.fit_transform(train_features):` Eğitim verilerini üç ana bileşene indirger ve yeni bileşene dönüştürür.

  `pca1 = pca_result[:,0]:` İlk ana bileşen vektörünü saklar.

  `pca2 = pca_result[:,1]:` İkinci ana bileşen vektörünü saklar.

  `pca3 = pca_result[:,2]:` Üçüncü ana bileşen vektörünü saklar.

  `target = pd.Categorical(y_train).codes:` y_train hedef değişkenini kategorik koda dönüştürür.

Bu kod, veriyi üç boyutta indirgerken hedef değişkeni de kategorik hale getirir.
"""

fig = plt.figure(figsize=(19, 7))

ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.set_title('PCA', fontsize=25, loc='left')
ax.scatter(xs=pca1,
           ys=pca2,
           zs=pca3,
           c=target,
           alpha=.9,)

ax.xaxis.line.set_color((1.0, 1.0, 1.0, 0.0))
ax.yaxis.line.set_color((1.0, 1.0, 1.0, 0.0))
ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0))


ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))

# ax.legend(['Poor','Average','Excellent'])

ax.legend(['Poor','Average','Excellent'], bbox_to_anchor=(1.02, -0.08)) # ['dusuk','ortalama','harika']

"""**MODELLEME**

En yüksek doğruluk ve F1 skorunu hangi modelin verdiğini değerlendirmek için birkaç farklı modeli uygun hale getirdim. Her modeli birbirine karşı doğru bir şekilde karşılaştırmak amacıyla, her model için en uygun hiperparametreleri belirlemek üzere bir grid-search gerçekleştirdim.
"""

def metrics_score(train_preds, y_train, test_preds, y_test):
    print(f"Training Accuracy:\t{accuracy_score(y_train, train_preds):.4}",
          f"\tTesting Accuracy:\t{accuracy_score(y_test, test_preds):.4}")
    print(f"Training Precision:\t{precision_score(y_train, train_preds, average='weighted'):.4}",
          f"\tTesting Precision:\t{precision_score(y_test, test_preds, average='weighted'):.4}")
    print(f"Training Recall:\t{recall_score(y_train, train_preds, average='weighted'):.4}",
          f"\tTesting Recall:\t\t{recall_score(y_test, test_preds, average='weighted'):.4}")
    print(f"Training F1:\t\t{f1_score(y_train, train_preds, average='weighted'):.4}",
          f"\tTesting F1:\t\t{f1_score(y_test, test_preds, average='weighted'):.4}")

"""**Naive Bayes**"""

scorer = make_scorer(f1_score, average='weighted')

params=  {'alpha': [0.09, 0.1,0.11]}

nb = MultinomialNB()

grid_search_nb = GridSearchCV(estimator = nb,scoring=scorer, param_grid = params,
                              cv = 5, n_jobs = -1, verbose = 2)

"""Multinomial Naive Bayes modelinin alpha hiperparametresini en uygun hale getirmek için GridSearchCV'yi kullanarak en iyi performansı elde etmeye çalışır."""

grid_search_nb.fit(train_features, y_train)

nb_train_preds = grid_search_nb.best_estimator_.predict(train_features)
nb_test_preds = grid_search_nb.best_estimator_.predict(test_features)
metrics_score(nb_train_preds, y_train, nb_test_preds, y_test)

"""**Logisitic Regression**"""

ss = StandardScaler()
train_scaled = ss.fit_transform(train_features)
test_scaled = ss.transform(test_features)

C = np.logspace(0, 1, 10)

params = dict(C=C, max_iter=[50, 100])

logistic = linear_model.LogisticRegression(penalty='l2', class_weight='balanced', random_state=42)

grid_search_LR = GridSearchCV(estimator = logistic,scoring=scorer, param_grid = params,
                              cv = 3, n_jobs = -1, verbose = 2)

"""Lojistik Regresyon modelinde, C parametresi regularizasyonun tersidir.

    Regularizasyon, modelin karmaşıklığını azaltmak ve aşırı uyumu (overfitting) önlemek için kullanılır.

    C değeri yüksekse, regularizasyon etkisi azalır ve model daha karmaşık olabilir.

    C değeri düşükse, regularizasyon etkisi artar ve model daha basit olur.

Logaritmik aralıkta değerler almak, farklı büyüklükteki C değerlerini denemeyi sağlar, böylece modelin en iyi performans gösterdiği regularizasyon gücünü bulunabilir. np.logspace(0, 1, 10) ifadesi, 1 ve 10 arasında 10 eşit aralıklı değeri oluşturur, bu da çeşitli regularizasyon seviyelerini kapsar.
"""

grid_search_LR.fit(train_scaled, y_train)

lr_train_preds = grid_search_LR.best_estimator_.predict(train_scaled)
lr_test_preds = grid_search_LR.best_estimator_.predict(test_scaled)
metrics_score(lr_train_preds, y_train, lr_test_preds, y_test)

"""**Logisitic Regression with PCA**"""

C = np.logspace(0, 1, 10)

params = dict(C=C, max_iter=[50, 100])

logistic = linear_model.LogisticRegression(penalty='l2', class_weight='balanced', random_state=42)

grid_search_LR_pca = GridSearchCV(estimator = logistic,scoring=scorer, param_grid = params,
                                  cv = 3, n_jobs = -1, verbose = 2)

grid_search_LR_pca.fit(pca_train, y_train)

lr_train_preds_pca = grid_search_LR_pca.best_estimator_.predict(pca_train)
lr_test_preds_pca = grid_search_LR_pca.best_estimator_.predict(pca_test)
metrics_score(lr_train_preds_pca, y_train, lr_test_preds_pca, y_test)

"""**Decision Tree**"""

criterion = ['gini', 'entropy']
max_depth = [2,4,6,8,10,12]

params = dict(criterion=criterion,
                max_depth=max_depth)

DT = tree.DecisionTreeClassifier(criterion =  'gini', max_depth= 10, random_state = 42)

grid_search_DT = GridSearchCV(estimator=DT, param_grid=params, scoring=scorer,
                              cv = 5, n_jobs = -1, verbose = 2)

grid_search_DT.fit(train_features, y_train)

dt_train_preds = grid_search_DT.best_estimator_.predict(train_features)
dt_test_preds = grid_search_DT.best_estimator_.predict(test_features)
metrics_score(dt_train_preds, y_train, dt_test_preds, y_test)

"""
**Decision Tree with PCA**"""

criterion = ['gini', 'entropy']
max_depth = [2,4,6,8,10,12]

params = dict(criterion=criterion,
                max_depth=max_depth)

DT = tree.DecisionTreeClassifier(criterion =  'gini', max_depth= 10, random_state = 42)

grid_search_DT_pca = GridSearchCV(estimator=DT, param_grid=params, scoring=scorer,
                                  cv = 5, n_jobs = -1, verbose = 2)

grid_search_DT_pca.fit(pca_train, y_train)

dt_train_preds_pca = grid_search_DT_pca.best_estimator_.predict(pca_train)
dt_test_preds_pca = grid_search_DT_pca.best_estimator_.predict(pca_test)
metrics_score(dt_train_preds_pca, y_train, dt_test_preds_pca, y_test)

"""**Random Forest**"""

params = {
    'n_estimators': [200, 500],
    'max_features': ['sqrt'],
    'max_depth' : [4,6,8],
    'criterion' :['gini', 'entropy']
}

rf = RandomForestClassifier(random_state = 42)

grid_search_RF = GridSearchCV(estimator=rf, param_grid=params, scoring=scorer,
                              cv = 5, n_jobs = -1, verbose = 2)

grid_search_RF.fit(train_features, y_train)

rf_train_preds = grid_search_RF.best_estimator_.predict(train_features)
rf_test_preds = grid_search_RF.best_estimator_.predict(test_features)
metrics_score(rf_train_preds, y_train, rf_test_preds, y_test)

"""**Light GBM**"""

lgbm = LGBMClassifier(random_state=42)

param_grid = {'n_estimators': [50, 100],
              'colsample_bytree': [0.7, 0.8],
              'max_depth': [15, 20],
              'num_leaves': [50, 100,],
              'min_split_gain': [0.3,0.4],
              'reg_alpha': [1.0, 1.1],
             'objective': ['multiclass'],
             'num_class':[3]
             }


grid_lgbm = GridSearchCV(lgbm, param_grid, cv=2, verbose=1, n_jobs=-1)

grid_lgbm.fit(train_features, y_train)

gbm_train_preds = grid_lgbm.best_estimator_.predict(train_features)
gbm_test_preds = grid_lgbm.best_estimator_.predict(test_features)
metrics_score(gbm_train_preds, y_train, gbm_test_preds, y_test)

"""**KNN**"""

knn = KNeighborsClassifier()
params = {
    'n_neighbors': [3,5,11,19],
    'weights': ['uniform', 'distance'],
    'metric': ['eucilidean', 'manhattan']
}

grid_knn = GridSearchCV(knn, params, verbose = 1,
                        cv = 3, n_jobs =-1)

grid_knn.fit(train_scaled, y_train)

knn_train_preds = grid_knn.best_estimator_.predict(train_scaled)
knn_test_preds = grid_knn.best_estimator_.predict(test_scaled)
metrics_score(knn_train_preds, y_train, knn_test_preds, y_test)

"""**MODEL DEĞERLENDİRME**"""

model_candidates = [

    {'name':'Naive Bayes',
     'accuracy score':accuracy_score(y_test, nb_test_preds),
     'f1 score':metrics.f1_score(y_test, nb_test_preds, average='weighted')},

    {'name':'Logistic Regression',
     'accuracy score':accuracy_score(y_test, lr_test_preds),
     'f1 score':metrics.f1_score(y_test, lr_test_preds, average='weighted')},

    {'name':'Logistic Regression (PCA)',
     'accuracy score':accuracy_score(y_test, lr_test_preds_pca),
    'f1 score':metrics.f1_score(y_test, lr_test_preds_pca, average='weighted')},

    {'name':'Decision Tree',
     'accuracy score':accuracy_score(y_test, dt_test_preds),
     'f1 score':metrics.f1_score(y_test, dt_test_preds, average='weighted')},

    {'name':'Decision Tree (PCA)',
     'accuracy score':accuracy_score(y_test, dt_test_preds_pca),
     'f1 score':metrics.f1_score(y_test, dt_test_preds_pca, average='weighted')},

    {'name':'Random Forest',
     'accuracy score':accuracy_score(y_test, rf_test_preds),
     'f1 score':metrics.f1_score(y_test, rf_test_preds, average='weighted')},

    {'name':'Light GBM',
     'accuracy score':accuracy_score(y_test, gbm_test_preds),
     'f1 score':metrics.f1_score(y_test, gbm_test_preds, average='weighted')},

    {'name':'KNN',
     'accuracy score':accuracy_score(y_test, knn_test_preds),
    'f1 score':metrics.f1_score(y_test, knn_test_preds, average='weighted')}

]

final_scores_df = pd.DataFrame(model_candidates).set_index('name')
final_scores_df

"""Light GBM modeli, en yüksek doğruluk ve F1 skoruna sahiptir.

**3 MODEL İÇİN CONFUSİON MATRİS**
"""

sns.color_palette("viridis", as_cmap=True)
# sns.set_context("talk")

fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(25, 10))

# plot logistic regression confusion matrix
sns.heatmap(confusion_matrix(y_test, lr_test_preds).T,
            square=True,
            annot=True,
            fmt='d',
            linewidths=1,
            vmin=200, vmax=1000, center=200,
            cmap="mako",
            ax=ax[0]).set_title('Logisitic Regression', fontsize=30)

ax[0].set_xlabel('Truth', fontsize=25)
ax[0].set_ylabel('Prediction', fontsize=25)
ax[0].set_xticklabels(['ortalama', 'harika', 'dusuk'], fontsize=15)
ax[0].set_yticklabels(['ortalama', 'harika', 'dusuk'], va='center', fontsize=15)

# plot logistic with PCA confusion matrix
sns.heatmap(confusion_matrix(y_test, lr_test_preds_pca).T,
            square=True,
            annot=True,
            fmt='d',
            linewidths=1,
            vmin=200, vmax=1000, center=200,
            cmap="mako",
            ax=ax[1]).set_title('Logisitic Regression (PCA)', fontsize=30)

ax[1].set_xlabel('Truth', fontsize=25)
ax[1].set_ylabel('Prediction', fontsize=25)
ax[1].set_xticklabels(['ortalama', 'harika', 'dusuk'], fontsize=15)
ax[1].set_yticklabels(['ortalama', 'harika', 'dusuk'], va='center', fontsize=15)
# plot logistic with PCA confusion matrix
sns.heatmap(confusion_matrix(y_test, lr_test_preds_pca).T,
            square=True,
            annot=True,
            fmt='d',
            linewidths=1,
            vmin=200, vmax=1000, center=200,
            cmap="mako",
            ax=ax[1]).set_title('Logisitic Regression (PCA)', fontsize=30)

ax[1].set_xlabel('Truth', fontsize=25)
ax[1].set_ylabel('Prediction', fontsize=25)
ax[1].set_xticklabels(['ortalama', 'harika', 'dusuk'], fontsize=15)
ax[1].set_yticklabels(['ortalama', 'harika', 'dusuk'], va='center', fontsize=15)

# plot LightGBM confusion matrix
sns.heatmap(confusion_matrix(y_test, gbm_test_preds).T,
            square=True,
            annot=True,
            fmt='d',
            linewidths=1,
            vmin=200, vmax=1000, center=200,
            cmap="mako",
            ax=ax[2]).set_title('LightGBM', fontsize=30)

ax[2].set_xlabel('gerçek', fontsize=25)
ax[2].set_ylabel('Prediction', fontsize=25)
ax[2].set_xticklabels(['ortalama', 'harika', 'dusuk'], fontsize=15)
ax[2].set_yticklabels(['ortalama', 'harika', 'dusuk'], va='center', fontsize=15)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Seaborn renk paleti ve genel ayarlar
sns.color_palette("viridis", as_cmap=True)

# Model isimleri ve tahmin sonuçları
models = ['Naive Bayes', 'LightGBM', 'KNN']
predictions = [nb_test_preds, gbm_test_preds, knn_test_preds]  # Tahmin sonuçları liste olarak

# Grafik oluşturma
fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(25, 10))

for i, (model_name, preds) in enumerate(zip(models, predictions)):
    sns.heatmap(confusion_matrix(y_test, preds).T,
                square=True,
                annot=True,
                fmt='d',
                linewidths=1,
                vmin=200, vmax=1000, center=200,
                cmap="mako",
                ax=ax[i]).set_title(model_name, fontsize=30)

    ax[i].set_xlabel('gerçek', fontsize=25)
    ax[i].set_ylabel('Prediction', fontsize=25)
    ax[i].set_xticklabels(['ortalama', 'harika', 'dusuk'], fontsize=15)
    ax[i].set_yticklabels(['ortalama', 'harika', 'dusuk'], va='center', fontsize=15)

plt.tight_layout()
plt.show()

"""Light GBM modeli, 'dusuk' incelemeleri en iyi şekilde belirlemiştir, oysa 'ortalama' ve 'mükemmel' incelemeler üç model arasında nispeten benzerdi. Bu nedenle, Light GBM nihai modelimizdir."""

import joblib

# 1. En iyi modeli ve parametrelerini elde edin
best_nb_model = grid_search_nb.best_estimator_

# 2. Modeli kaydet
model_path = "best_multinomial_nb_model.pkl"
joblib.dump(best_nb_model, model_path)

# 3. TF-IDF vektörleştiriciyi de kaydet
tfidf_vectorizer_path = "tfidf_vectorizer.pkl"
joblib.dump(tfidf_train, tfidf_vectorizer_path)

print("Model ve TF-IDF vektörleştirici başarıyla kaydedildi!")

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. TF-IDF vektörleştiriciyi oluşturun ve eğitim verisini dönüştürün
tfidf_train = TfidfVectorizer(sublinear_tf=True, max_df=0.9, min_df=0.05, ngram_range=(1, 1))
train_features = tfidf_train.fit_transform(X_train).toarray()

# 2. Modeli eğitin (örnek LightGBM modeli

# 3. TF-IDF vektörleştiriciyi ve modeli kaydedin
joblib.dump(tfidf_train, "tfidf_vectorizer.pkl")
joblib.dump(grid_lgbm.best_estimator_, model_path)
print("Model ve TF-IDF vektörleştirici başarıyla kaydedildi!")